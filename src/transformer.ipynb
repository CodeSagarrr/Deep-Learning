{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf8318aa",
   "metadata": {},
   "source": [
    "# Transformers Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd522040",
   "metadata": {},
   "source": [
    "* Transformer ek deep learning architecture hai jo sequence data ko samajhne me expert hota hai jaise text, audio, code. Ye RNN ya LSTM ki tarah step by step process nahi karta balki pura sequence ek saath dekhta hai.\n",
    "\n",
    "* Iska core idea attention hota hai jisme model har word ko baaki words ke context me samajhta hai. Isse yaad rakhne ki problem khatam ho jaati hai.\n",
    "\n",
    "* Transformer encoder part input ka meaning samajhta hai aur decoder part output create karta hai, isi se translation models aur large language models ban sakte hain.\n",
    "\n",
    "* Ye design parallel processing ko allow karta hai jisse training fast hoti hai aur large datasets ko easily handle kiya ja sakta hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ab65c",
   "metadata": {},
   "source": [
    "### Why in AI/ML :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb48af",
   "metadata": {},
   "source": [
    "* Transformer context ko deep level par samajh pata hai. Sentence me koi word shuru me ho ya last me, attention mechanism uska relation identify kar leta hai.\n",
    "\n",
    "* Ye long sequence understanding me perfect hota hai. RNN aur LSTM long sentences me confuse ho jaate hain, transformer nahi hota.\n",
    "\n",
    "* Real world me translation, chatbots, sentiment analysis, document understanding, coding assistants, vision tasks sab me transformer backbone ka role hota hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6329b75",
   "metadata": {},
   "source": [
    "### Important components intuition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859d8a4",
   "metadata": {},
   "source": [
    "* Self Attention :\n",
    "\n",
    "    - Ye mechanism har word ko sentence ke baaki words se compare karta hai.\n",
    "\n",
    "    - Example intuition:\n",
    "\n",
    "        - Sentence: \"The cat sat on the mat because it was warm\"\n",
    "\n",
    "        - \"it\" kis ko refer karta hai?\n",
    "\n",
    "        - Attention pehle \"cat\" par focus karega kyunki relation strong hai.\n",
    "\n",
    "    - Self attention formula values, keys, queries se milkar banta hai jisme model decision leta hai ki kis word ko kitna weight dena chahiye."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ed3c5",
   "metadata": {},
   "source": [
    "### Positional Encoding :\n",
    "\n",
    "* Transformer order nahi samajhta. Isse har word ko ek special pattern milta hai jisse model sequence order samajhta hai.\n",
    "\n",
    "* Ex. :\n",
    "\n",
    "    - Word1 = sine cosine pattern\n",
    "\n",
    "    - Word2 = different pattern\n",
    "\n",
    "    - Isse model janta hai ki kaunsa word pehle aur kaunsa baad me aaya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea3bf8",
   "metadata": {},
   "source": [
    "### Multi Head Attention :\n",
    "\n",
    "* Ek attention ek cheez dekhega, multiple heads multiple relationships dekhte hain.\n",
    "\n",
    "* Ek head syntax samajh sakta hai\n",
    "\n",
    "* Ek head noun adjective relation\n",
    "\n",
    "* Ek head long range dependency\n",
    "\n",
    "* Sab combine hoke deep understanding milti hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88b4a0",
   "metadata": {},
   "source": [
    "### Feed Forward Network :\n",
    "\n",
    "* Attention ke baad simple dense layers features ko refine karte hain. Yeh part meaning ko aur clean karta hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2694c0a",
   "metadata": {},
   "source": [
    "#### Simple transformer encoder block intuition code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc34143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes :  torch.Size([10, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/technicalvishwakarmaji/Downloads/React/Deep-Learning/venv/lib/python3.14/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    # word vector size\n",
    "    d_model=512,\n",
    "    # multi head attention\n",
    "    nhead=8\n",
    ")\n",
    "\n",
    "encoder = nn.TransformerEncoder(encoder_layer , num_layers=2)\n",
    "\n",
    "# 10 = sequence length\n",
    "# 1 = batch size\n",
    "# 512 = feature size\n",
    "\n",
    "x = torch.randn(10 , 1 , 512)\n",
    "\n",
    "out = encoder(x)\n",
    "print(\"Shapes : \" , out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd861b",
   "metadata": {},
   "source": [
    "* Transformer encoder input me har token ko 512 dimension me vector ke form me leta hai.\n",
    "\n",
    "* 8 heads input ke alag alag relations parallel me analyze karte hain.\n",
    "\n",
    "* 2 layers ka matlab 2 baar deep attention understanding hogi.\n",
    "\n",
    "* Output shape same hota hai kyunki transformer sequence ko transform karta hai, shrink nahi karta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39173c39",
   "metadata": {},
   "source": [
    "# Attention Concept :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27db4dd",
   "metadata": {},
   "source": [
    "* Attention ek aisa mechanism hota hai jisme model input ke har word ko baaki words ke relation me dekhta hai. Model ye decide karta hai ki kis word par focus karna zaroori hai aur kis par kam.\n",
    "\n",
    "* Ye weight system jaisa hota hai. Important words ko high weight milta hai aur non important words ko low weight. Isse sentence ka real meaning clear ho jata hai.\n",
    "\n",
    "* Attention query, key aur value vector use karta hai. Query ek word ko represent karta hai, key context deta hai aur value actual information hoti hai.\n",
    "\n",
    "* Attention ka final output weighted sum hota hai jisse model ko contextual understanding milti hai jaise word kis se judta hai, kya meaning deta hai aur sentence me kya role hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af214c9",
   "metadata": {},
   "source": [
    "* Why Attention important in AI/ML :\n",
    "\n",
    "    - Sentence me long distance relation identify karna easy ho jata hai. Example: \"The dog that chased the cat was hungry.\" Yaha \"dog\" aur \"hungry\" ka relation attention turant pakad leta hai.\n",
    "\n",
    "    - Attention model ko decide karne deta hai ki kis part ko highlight karna hai. Isse model flexible aur smart ho jata hai.\n",
    "\n",
    "    - Ye parallel process hota hai jisse training fast hoti hai aur large sequence efficiently handle ho jata hai.\n",
    "\n",
    "    - Real world me translation, summarization, QA, chatbots sab attention concept ke bina impossible hain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82a1d7",
   "metadata": {},
   "source": [
    "* Why Transformer Beats RNN :\n",
    "\n",
    "    - Transformers poori sequence ko ek saath process karte hain, RNN step by step chalti hai. Isse transformer fast hota hai aur GPU par parallel training possible hoti hai.\n",
    "\n",
    "    - RNN long sentences me memory lose kar deta hai. Transformer attention ki wajah se long range relation perfect samajh leta hai.\n",
    "\n",
    "    - RNN me gradient vanish aur explode problem common hoti hai. Transformer ki attention architecture ye problem almost remove kar deti hai.\n",
    "\n",
    "    - Transformer ki multi head attention diverse relations capture karti hai. RNN ek baar me ek pattern focus karta hai. Isliye transformer deeper, richer understanding deta hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ae02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Shape :  torch.Size([1, 10, 10])\n",
      "weights :  tensor([[[0.0626, 0.3061, 0.0382, 0.0558, 0.0549, 0.1004, 0.0743, 0.0686,\n",
      "          0.1158, 0.1233],\n",
      "         [0.0660, 0.1007, 0.1701, 0.0915, 0.0942, 0.1478, 0.1223, 0.0703,\n",
      "          0.0479, 0.0892],\n",
      "         [0.0770, 0.0664, 0.2037, 0.1080, 0.1336, 0.1089, 0.0847, 0.1126,\n",
      "          0.0501, 0.0551],\n",
      "         [0.1166, 0.0980, 0.1191, 0.1305, 0.0652, 0.0625, 0.0830, 0.1158,\n",
      "          0.1242, 0.0852],\n",
      "         [0.1067, 0.1805, 0.1572, 0.0457, 0.0714, 0.0833, 0.1001, 0.0772,\n",
      "          0.0652, 0.1126],\n",
      "         [0.0454, 0.1487, 0.1179, 0.0732, 0.0689, 0.1364, 0.1197, 0.1037,\n",
      "          0.0947, 0.0913],\n",
      "         [0.1401, 0.0575, 0.0734, 0.1147, 0.0814, 0.0431, 0.2510, 0.0864,\n",
      "          0.0951, 0.0573],\n",
      "         [0.1763, 0.1109, 0.1233, 0.0747, 0.1359, 0.0957, 0.0468, 0.0745,\n",
      "          0.0544, 0.1075],\n",
      "         [0.1119, 0.0924, 0.0357, 0.1069, 0.0967, 0.1099, 0.0321, 0.1522,\n",
      "          0.1546, 0.1075],\n",
      "         [0.0857, 0.0689, 0.0783, 0.1160, 0.1644, 0.0831, 0.1603, 0.0706,\n",
      "          0.1010, 0.0716]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "attn = nn.MultiheadAttention(embed_dim=64 , num_heads=1)\n",
    "x = torch.randn(10 , 1 , 64)\n",
    "\n",
    "out , weights = attn(x , x , x)\n",
    "\n",
    "print(\"Weights Shape : \" , weights.shape)\n",
    "print(\"weights : \" , weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9fc95",
   "metadata": {},
   "source": [
    "* weights me attention scores hoti hain jisse model ke focus ka exact idea mil jata hai.\n",
    "\n",
    "* Ye batata hai ki sequence ka har token kis token se kitna connected hai.\n",
    "\n",
    "* Transformers isi mechanism se RNN ki limitation ko cross karte hain aur richer context produce karte hain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
