{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05901696",
   "metadata": {},
   "source": [
    "### Small Feed-Forward Model + Train on Dummy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f4b2d",
   "metadata": {},
   "source": [
    "* Feed-forward neural network ek simple deep learning model hota hai jisme data input se output tak straight flow me jata hai, koi loop ya memory nahi hoti.\n",
    "\n",
    "* Model multiple linear layers + activation functions ka combination hota hai jisse complex patterns learn karne ki power milti hai.\n",
    "\n",
    "* Har layer input features ko weights ke through transform karti hai aur network step-by-step data ko meaningful representation me convert karta hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f96b78",
   "metadata": {},
   "source": [
    "## # Dummy Dataset Banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9b647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/technicalvishwakarmaji/Downloads/React/Deep-Learning/venv/lib/python3.14/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# 200 samples, har sample me 4 features random values ke sath\n",
    "X = torch.randn(200 , 4)\n",
    "\n",
    "# 0 aur 1 ke random labels, binary classification\n",
    "y = torch.randint(0 , 2 , (200,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ad6f8",
   "metadata": {},
   "source": [
    "## # DataLoader Banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe260c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset , DataLoader\n",
    "\n",
    "# dataset combine kar diya inputs aur labels ko\n",
    "dataset = TensorDataset(X , y)\n",
    "\n",
    "# training batches me hoga 16 samples ek baar me\n",
    "loader = DataLoader(dataset , batch_size=16 , shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255f100",
   "metadata": {},
   "source": [
    "## # Feed-Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1082912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # 4 features ko 16 neurons me convert kar raha hai\n",
    "            nn.Linear(4 , 16),\n",
    "\n",
    "            # non-linear activation jisse model zyada pattern learn kare\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # hidden layer output ko 8 neurons me reduce kar raha hai\n",
    "            nn.Linear(16 , 8),\n",
    "\n",
    "            # network ko deep banata hai\n",
    "            nn.ReLU(),\n",
    "\n",
    "            #  final 2 outputs: class 0 ya class 1\n",
    "            nn.Linear(8 , 2)\n",
    "        )\n",
    "\n",
    "# yaha se model prediction return karta hai\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb02d85",
   "metadata": {},
   "source": [
    "## # model create:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfffbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213dc612",
   "metadata": {},
   "source": [
    "## # Loss + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86f18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax + cross entropy combined, classification me most common\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam fast learning optimizer\n",
    "opt = torch.optim.Adam(model.parameters() , lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27dbc5",
   "metadata": {},
   "source": [
    "## # Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc :  1 Loss : 0.6695436835289001\n",
      "epoc :  1 Loss : 0.7028952240943909\n",
      "epoc :  1 Loss : 0.6392556428909302\n",
      "epoc :  1 Loss : 0.7500025629997253\n",
      "epoc :  1 Loss : 0.6961150169372559\n",
      "epoc :  1 Loss : 0.737363874912262\n",
      "epoc :  1 Loss : 0.5829362273216248\n",
      "epoc :  1 Loss : 0.7121781706809998\n",
      "epoc :  1 Loss : 0.6931735873222351\n",
      "epoc :  1 Loss : 0.7343351244926453\n",
      "epoc :  1 Loss : 0.6909631490707397\n",
      "epoc :  1 Loss : 0.6801091432571411\n",
      "epoc :  1 Loss : 0.6947095394134521\n",
      "epoc :  2 Loss : 0.6966687440872192\n",
      "epoc :  2 Loss : 0.6648796796798706\n",
      "epoc :  2 Loss : 0.7607920169830322\n",
      "epoc :  2 Loss : 0.7049949169158936\n",
      "epoc :  2 Loss : 0.6388078331947327\n",
      "epoc :  2 Loss : 0.723746657371521\n",
      "epoc :  2 Loss : 0.7239412069320679\n",
      "epoc :  2 Loss : 0.5713310837745667\n",
      "epoc :  2 Loss : 0.7226255536079407\n",
      "epoc :  2 Loss : 0.6928966045379639\n",
      "epoc :  2 Loss : 0.6364867687225342\n",
      "epoc :  2 Loss : 0.7215549945831299\n",
      "epoc :  2 Loss : 0.6606445908546448\n",
      "epoc :  3 Loss : 0.662097156047821\n",
      "epoc :  3 Loss : 0.6868762373924255\n",
      "epoc :  3 Loss : 0.7638339400291443\n",
      "epoc :  3 Loss : 0.684676468372345\n",
      "epoc :  3 Loss : 0.6473171710968018\n",
      "epoc :  3 Loss : 0.6650121212005615\n",
      "epoc :  3 Loss : 0.752601683139801\n",
      "epoc :  3 Loss : 0.6118883490562439\n",
      "epoc :  3 Loss : 0.6821966171264648\n",
      "epoc :  3 Loss : 0.7050454020500183\n",
      "epoc :  3 Loss : 0.6767609715461731\n",
      "epoc :  3 Loss : 0.6565691232681274\n",
      "epoc :  3 Loss : 0.7310154438018799\n",
      "epoc :  4 Loss : 0.6634588837623596\n",
      "epoc :  4 Loss : 0.7286816239356995\n",
      "epoc :  4 Loss : 0.6350311040878296\n",
      "epoc :  4 Loss : 0.7186739444732666\n",
      "epoc :  4 Loss : 0.6931770443916321\n",
      "epoc :  4 Loss : 0.7119718194007874\n",
      "epoc :  4 Loss : 0.7063776254653931\n",
      "epoc :  4 Loss : 0.694043755531311\n",
      "epoc :  4 Loss : 0.7034243941307068\n",
      "epoc :  4 Loss : 0.6882016062736511\n",
      "epoc :  4 Loss : 0.6976773738861084\n",
      "epoc :  4 Loss : 0.5874137878417969\n",
      "epoc :  4 Loss : 0.5920802354812622\n",
      "epoc :  5 Loss : 0.6907243132591248\n",
      "epoc :  5 Loss : 0.6643097996711731\n",
      "epoc :  5 Loss : 0.6615062952041626\n",
      "epoc :  5 Loss : 0.7114373445510864\n",
      "epoc :  5 Loss : 0.6630417108535767\n",
      "epoc :  5 Loss : 0.682163417339325\n",
      "epoc :  5 Loss : 0.6747870445251465\n",
      "epoc :  5 Loss : 0.7168487906455994\n",
      "epoc :  5 Loss : 0.6827427744865417\n",
      "epoc :  5 Loss : 0.6774603128433228\n",
      "epoc :  5 Loss : 0.6187629103660583\n",
      "epoc :  5 Loss : 0.7196065783500671\n",
      "epoc :  5 Loss : 0.6619852781295776\n"
     ]
    }
   ],
   "source": [
    "for epoc in range(5):\n",
    "    for batch_x , batch_y in loader:\n",
    "\n",
    "        # forward pass: model se prediction aaya\n",
    "        pred = model(batch_x)\n",
    "\n",
    "        # loss calculate: prediction kitna sahi ya galat\n",
    "        loss = loss_fn(pred , batch_y)\n",
    "\n",
    "        # pehle wale gradients clear\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # new gradients calculate\n",
    "        loss.backward()\n",
    "\n",
    "        # weights update\n",
    "        opt.step()\n",
    "\n",
    "        print(\"epoc : \" , epoc+1 , \"Loss :\" , loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800741d",
   "metadata": {},
   "source": [
    "## # Important Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe29da",
   "metadata": {},
   "source": [
    "* Feed-forward model ke outputs logits hote hain, CrossEntropyLoss automatically softmax apply karta hai isliye separate softmax lagana nahi hota.\n",
    "\n",
    "* Batch training se model stable hota hai, pure dataset ek sath pass karna training ko slow aur unstable bana deta hai.\n",
    "\n",
    "* Model training ke baad evaluation mode me model.eval() use hota hai jisse unnecessary gradient tracking off ho jati hai.\n",
    "\n",
    "* Dummy dataset training beginners ko internal working samjhati hai, phir wahi code real datasets par as-is apply hota hai without changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
