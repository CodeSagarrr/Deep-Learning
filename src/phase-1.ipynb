{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c087ef64",
   "metadata": {},
   "source": [
    "# Deep Learning + NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc14ec",
   "metadata": {},
   "source": [
    "* Goal: Basic neural network understanding + run deep models (not training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea40536",
   "metadata": {},
   "source": [
    "## Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05853f",
   "metadata": {},
   "source": [
    "* Ek artificial neuron ek simple computational unit hai jo kuch inputs leta hai, unko weight se scale karta hai, ek bias add karta hai, aur fir ek activation function se pass karta hai. Ye biological neuron ka bahut simplified model hai.\n",
    "\n",
    "* Why in AI/ML : \n",
    "\n",
    "    - Individual neurons simple transformation karte hain. Jab bahut sare neurons ko jodte hain, network complex patterns aur features ko represent kar sakta hai. Neuron se feature extraction aur nonlinearity aati hai.\n",
    "\n",
    "    - Weights = neuron ki memory / parameters.\n",
    "\n",
    "    - Bias = threshold ko shift karta hai.\n",
    "\n",
    "    - Activation = nonlinearity; bina activation ke stacked linear layers ek hi linear function ho jayenge, isliye activation zaroori hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c49b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z (linear output) = 1.3199999999999998\n",
      "y (after ReLU) = 1.3199999999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# single input vector (for example ek data point ke features)\n",
    "x = np.array([0.5, -1.2, 2.0]) #3 features\n",
    "\n",
    "# weights aur bias (ye model ke parameters hain)\n",
    "w = np.array([0.4, -0.6, 0.1])\n",
    "b= 0.2\n",
    "\n",
    "# linear combination: z = w.x + b\n",
    "z = np.dot(w , x) + b\n",
    "\n",
    "# activation: y = relu(z) (ReLU = max(0, z)) \n",
    "y = max(0 , z)\n",
    "\n",
    "print(\"z (linear output) =\", z)\n",
    "print(\"y (after ReLU) =\", y)\n",
    "\n",
    "# - np.dot(w, x) computes weighted sum.\n",
    "# - ReLU adds nonlinearity and helps model complex relationships. The ReLU (Rectified Linear Unit) is a popular activation function in neural networks defined by the formula \\(f(x)=\\max (0,x)\\). It is computationally efficient and introduces non-linearity by outputting the input \\(x\\) if it is positive and \\(0\\) if it is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e3a14",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4857eca",
   "metadata": {},
   "source": [
    "* Layer neurons ka ek group hota hai jo ek saath operate karte hain. Input layer, hidden layers, aur output layer common hain. Har layer multiple neurons ka weighted sum aur activation perform karti hai.\n",
    "\n",
    "* Why in AI/ML :\n",
    "\n",
    "    - Layers hierarchical features banate hain. Pehli layers low-level features detect karte hain (edges, blobs), agle layers higher-level concepts (shapes, objects). Layering se network deep features seekh sakta hai.\n",
    "\n",
    "* Types :\n",
    "\n",
    "    - Dense / Fully connected layer: har neuron previous layer ke sab inputs leta hai.\n",
    "\n",
    "    - Convolutional layer: local patterns pe focus karta hai (images).\n",
    "\n",
    "    - Recurrent / Transformer layers: sequence data ke liye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z shape: (2, 3)\n",
      "Y shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# batch of 2 samples, each with 4 features\n",
    "\n",
    "X = np.array([[0.1, 0.2, 0.3, 0.4],\n",
    "              [1.0, -0.5, 0.0, 0.2]])\n",
    "\n",
    "#  weights shape: (input_dim, units)\n",
    "\n",
    "w = np.random.randn(4 , 3) * 0.1 #  3 neurons in this layer\n",
    "b = np.zeros(3)  # biases for 3 neurons\n",
    "\n",
    "# linear combination: Y = X.w + b\n",
    "Z = np.dot(X , w) + b\n",
    "\n",
    "# apply activation (sigmoid for example)\n",
    "Y = 1 / (1 + np.exp(-Z))\n",
    "\n",
    "print(\"Z shape:\", Z.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "\n",
    "# - Each row of Y is the output for one sample.\n",
    "# - We used matrix multiplication to compute all neurons in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c708ec2",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b3f22",
   "metadata": {},
   "source": [
    "* Activation function neuron ke linear output ko transform karne wali function hoti hai jisse model non-linear patterns learn kar sake.\n",
    "\n",
    "* Agar layers ke beech nonlinearity nahi hogi, to stacked layers ka combination ek hi linear mapping hi rahega. Nonlinearity hi complex decision boundaries banati hai.\n",
    "\n",
    "* Common activations :\n",
    "\n",
    "    - ReLU (Rectified Linear Unit) = max(0, x). Simple, fast, sparse activation. Bahut common for hidden layers.\n",
    "\n",
    "    - Sigmoid = 1 / (1 + e^-x). Output 0-1. Use: binary probability output (but hidden layers me kabhi vanishing gradient problem kar sakta hai).\n",
    "\n",
    "    - Tanh = (e^x - e^-x)/(e^x + e^-x). Output -1 to 1. Centered at zero.\n",
    "\n",
    "    - Softmax = scores ko probability distribution me convert karta hai (multi-class output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fdb5c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU: [[0.  0.  0.  0.5 2. ]]\n",
      "Sigmoid: [[0.11920292 0.37754067 0.5        0.62245933 0.88079708]]\n",
      "Tanh: [[-0.96402758 -0.46211716  0.          0.46211716  0.96402758]]\n",
      "Softmax: [[0.01255471 0.0562663  0.09276745 0.15294766 0.68546388]]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0 , x)\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "     # numerically stable softmax for 2D array (batch)\n",
    "     ex = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "     return ex / np.sum(ex, axis=1, keepdims=True)\n",
    "\n",
    "x = np.array([[-2.0, -0.5, 0.0, 0.5, 2.0]])\n",
    "\n",
    "print(\"ReLU:\", relu(x))\n",
    "print(\"Sigmoid:\", sigmoid(x))\n",
    "print(\"Tanh:\", tanh(x))\n",
    "print(\"Softmax:\", softmax(x))\n",
    "\n",
    "# - Softmax shown for batch input shape (1,5). It converts to probabilities summing to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05481f73",
   "metadata": {},
   "source": [
    "# deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3ca3c",
   "metadata": {},
   "source": [
    "* Hierarchical feature learning: Deep networks layers ke through simple features se complex features banate hain. Example: image me pehle layer edges detect karegi, beech ki layers patterns, top layers objects. Is combination se complex tasks solve hote hain.\n",
    "\n",
    "* Nonlinearity + depth = expressive power: Nonlinear activations allow layers ko complex functions approximate karne me help. Zyada layers se model complicated mappings sikhta hai.\n",
    "\n",
    "* Data driven representation: Classical ML me features manually design karne padte the. Deep learning raw data se khud features learn karta hai (feature engineering ki zaroorat kam hoti hai).\n",
    "\n",
    "* Scale with data and computation: Large datasets aur GPU compute se networks ko zyada accurate bana sakte ho.\n",
    "\n",
    "* Optimization and regularization tricks: Techniques jaise batch normalization, dropout, and good optimizers (Adam) training ko stable karte hain. Ye sab practical techniques hain jo deep learning ko feasible banati hain.\n",
    "\n",
    "* Transfer learning: Pretrained deep models kisi ek task pe train karke dusre similar tasks me reuse ho sakte hain, jisse training data kam me bhi acha result milega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c44982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_inference_demo.py\n",
    "# Install requirement: pip install tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 1) Define a simple sequential model\n",
    "# - Input shape = 20 features\n",
    "# - Two hidden dense layers with ReLU\n",
    "# - Output layer with softmax for 3-class classification\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(20,), name=\"input_layer\"),\n",
    "    layers.Dense(64, activation=\"relu\", name=\"hidden_dense_1\"),\n",
    "    layers.Dense(32, activation=\"relu\", name=\"hidden_dense_2\"),\n",
    "    layers.Dense(3, activation=\"softmax\", name=\"output_layer\")\n",
    "])\n",
    "\n",
    "# 2) Print model summary to see parameters and shapes\n",
    "model.summary()\n",
    "# Comments:\n",
    "# - Dense layers contain weights and biases that will be learned during training.\n",
    "# - ReLU in hidden layers introduces nonlinearity.\n",
    "# - Softmax in output layer converts scores to probabilities for 3 classes.\n",
    "\n",
    "# 3) Create random input and run a forward pass (inference)\n",
    "# Suppose batch of 5 samples, each with 20 features\n",
    "x_random = np.random.randn(5, 20).astype(np.float32)\n",
    "\n",
    "# Predict (forward pass). No training involved here.\n",
    "predictions = model(x_random, training=False)  # or model.predict(x_random)\n",
    "\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "print(\"Predictions (probabilities):\\n\", predictions.numpy())\n",
    "\n",
    "# Explanation:\n",
    "# - model(x_random, training=False) performs computation through all layers.\n",
    "# - Output is a (5,3) array of probabilities for each sample.\n",
    "# - We did not call compile or fit because we are only demonstrating forward pass.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
