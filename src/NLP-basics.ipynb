{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47561318",
   "metadata": {},
   "source": [
    "# NLP basics :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f6c27",
   "metadata": {},
   "source": [
    "* NLP yani Natural Language Processing ek field hai jisme computer ko human language samajhna aur generate karna sikhaya jata hai. Ye text ko numbers me convert karke machine ko meaning samajhne layak banata hai.\n",
    "\n",
    "* NLP me grammar, context, aur intent jaise factors ko model seekh kar text ka deep relation samajhta hai. Ye simple keyword matching se kai zyada smart hota hai.\n",
    "\n",
    "* NLP ka goal language ko machine friendly format me break karna hai taaki model text classify, translate, summarize ya predict kar sake.\n",
    "\n",
    "* NLP multiple layers ka process hota hai jaise tokenization, embedding, context modelling aur inference jisse final meaningful output milta hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7927cde",
   "metadata": {},
   "source": [
    "### 1# Tokenization :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a2f71",
   "metadata": {},
   "source": [
    "* Tokenization ek process hai jisme sentence ko chote manageable units me break kiya jata hai jaise words, subwords ya characters.\n",
    "\n",
    "* Ye raw text ko balanced pieces me split karta hai taaki model sequence ko step by step samajh sake.\n",
    "\n",
    "* Modern NLP me subword tokenization famous hai kyunki ye rare words ko bhi break karke handle kar leta hai.\n",
    "\n",
    "* Tokenization model ke input ka base hota hai kyunki bina clean tokens model correct attention flow nahi seekh pata.\n",
    "\n",
    "* Why in AI/ML :\n",
    "\n",
    "    - Tokenization text ko numerical modelling ke layak banata hai jisse ML model sequence ko proper order me process kar pata hai.\n",
    "\n",
    "    - Ye word variation, spelling change aur rare words ko handle karta hai jo training stability improve karta hai.\n",
    "\n",
    "    - Bina tokenization model complete sentence ko ek single block ke tarah dekhega, jisse context aur meaning lose ho jayega."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7eafc3",
   "metadata": {},
   "source": [
    "* Types :\n",
    "\n",
    "    - Word tokenization\n",
    "\n",
    "    - Character tokenization\n",
    "\n",
    "    - Subword tokenization (BPE, WordPiece, SentencePiece)\n",
    "\n",
    "        - Subword sabse powerful hota hai kyunki new words ko bhi model seekh sakta hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e982d54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'helps', 'ai', 'understand', 'language']\n",
      "[101, 3698, 4083, 7126, 9932, 3305, 2653, 102]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer , AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Machine learning helps AI understand language\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)  # text ko subword tokens me break karta hai\n",
    "ids = tokenizer.encode(text)  # tokens ko numeric ids me convert karta hai\n",
    "\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a0acf",
   "metadata": {},
   "source": [
    "### 2# Embeddings :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2bbb1",
   "metadata": {},
   "source": [
    "* Embeddings ek vector representation hoti hai jisme har word ko ek dense (close) numeric space me convert kiya jata hai.\n",
    "\n",
    "* Ye numerical vectors words ke meaning ka relationship capture karte hain jaise king queen similarity.\n",
    "\n",
    "* Embeddings high dimensional data ko compressed form me store karte hain jisse model context aur semantics samajh pata hai.\n",
    "\n",
    "* Embeddings dynamic hoti hain yani model training ke sath improve hoti jati hain aur language understand karne me smarter ban jati hain.\n",
    "\n",
    "* Why in AI/ML :\n",
    "\n",
    "    - Embeddings machine ko deep meaning aur context samajhne ki ability deti hai jo AI models ka core hota hai.\n",
    "\n",
    "    - Ye ML models ko long term patterns aur relation seekhne me help karti hain jaise synonyms, analogies.\n",
    "\n",
    "    - Embeddings ke bina text vectors sparse aur inefficient hote jisse model learn nahi kar pata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d0812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.6162e-01, -1.5060e-01, -5.8002e-01, -1.9405e-01, -3.7837e-01,\n",
      "         -2.6904e-01,  4.5140e-01,  3.2322e-01, -1.7788e-01, -2.7300e-01,\n",
      "         -2.5873e-01,  1.2740e-02, -1.3905e-01,  1.1248e-01,  4.4079e-01,\n",
      "          2.4274e-01, -1.8626e-01,  4.9890e-01,  2.0663e-01, -3.5341e-01,\n",
      "         -1.0902e-02, -4.5293e-01, -6.6118e-01, -5.5842e-01,  2.0922e-01,\n",
      "          6.0444e-02,  6.8188e-02, -4.9698e-01,  1.5289e-01,  1.6451e-01,\n",
      "         -2.3054e-01,  1.5929e-01, -3.6369e-01, -4.2262e-02,  5.3762e-01,\n",
      "         -1.3789e-01,  3.8621e-01, -1.3518e-01,  2.5008e-01,  6.3026e-02,\n",
      "         -2.4635e-01, -1.7674e-01,  7.5943e-01, -3.9611e-02, -4.0518e-01,\n",
      "         -1.8297e-01, -3.1206e+00, -9.6969e-02, -6.6714e-01, -5.3219e-01,\n",
      "         -3.2492e-01,  3.1801e-03,  2.7319e-02,  8.6779e-01,  1.2414e-03,\n",
      "          5.4698e-01, -4.4062e-02,  4.7390e-01,  2.4350e-01,  8.1779e-02,\n",
      "          9.4857e-02,  7.8707e-02, -1.3740e-01,  1.4905e-01,  1.7796e-01,\n",
      "          2.0958e-01, -1.4692e-01,  5.2949e-01, -6.4145e-01,  2.8332e-01,\n",
      "         -6.7306e-01, -2.8772e-01,  2.2573e-01, -4.8392e-02, -2.4851e-01,\n",
      "         -1.5372e-02, -2.4361e-01,  2.2892e-01, -4.6883e-01, -4.7970e-01,\n",
      "         -3.5082e-01,  5.2231e-01,  4.7842e-01, -2.1082e-01,  8.6977e-01,\n",
      "          4.1364e-01, -2.9665e-01, -1.1917e-01, -2.1159e-02,  8.9978e-01,\n",
      "          1.0394e-01, -1.7880e-01,  6.6415e-02,  5.2623e-01,  3.5815e-01,\n",
      "          2.5035e-01, -1.2895e-01, -3.6993e-02,  4.0304e-01,  2.7456e-01,\n",
      "         -2.4017e-01, -6.2648e-01,  2.2851e-01, -6.3285e-01, -8.5026e-02,\n",
      "         -2.4629e-01, -4.8787e-01, -1.8852e-01, -1.2368e-01, -2.5002e+00,\n",
      "          1.1862e-01,  2.8642e-02, -3.6142e-01, -1.8548e-02,  1.6024e-01,\n",
      "          5.3980e-01,  8.9551e-02,  1.1223e-01,  3.4772e-01,  1.6122e-01,\n",
      "         -2.2039e-02,  5.9991e-01, -3.2455e-01, -9.9264e-02,  3.5920e-01,\n",
      "          1.7285e-01, -1.7118e-01,  7.9702e-02,  4.5381e-01,  3.8546e-01,\n",
      "          5.2511e-04,  7.8705e-01,  3.1360e-01, -3.4406e-01, -7.1224e-01,\n",
      "         -2.2440e-01,  2.4907e-01,  2.7443e-01, -3.1471e-01, -2.9796e-01,\n",
      "         -6.1929e-01, -3.0155e-01, -2.5746e+00, -2.6484e-01,  1.2784e+00,\n",
      "         -7.0947e-02, -2.3629e-01, -2.0499e-01,  3.4072e-01,  3.6055e-01,\n",
      "          2.4211e-02,  3.2350e-02, -1.0943e-02,  4.5185e-02, -6.2834e-01,\n",
      "          1.1892e-01, -1.1804e-02, -1.9244e-03,  1.9488e-01,  3.4500e-01,\n",
      "          2.4663e-01,  1.2741e-01, -8.5775e-02, -4.4676e-01, -3.6032e-01,\n",
      "         -1.1813e-02,  2.9993e-01,  3.2534e-01,  3.2308e-01, -3.9019e-01,\n",
      "         -9.6827e-02, -2.6744e-01,  6.2415e-01,  2.2598e-01,  5.4353e-01,\n",
      "          3.6682e-02, -6.5303e-01, -1.8132e-02,  4.6849e-01, -1.0067e-01,\n",
      "         -3.2456e-01,  4.9250e-01,  1.6127e-01, -1.5940e-02,  2.3820e-01,\n",
      "         -3.5967e-02,  9.8262e-02, -1.7013e-01, -9.2294e-02, -6.9853e-02,\n",
      "         -2.5528e-01, -1.5869e-01,  1.3820e-01, -4.1162e-01,  5.2695e-01,\n",
      "          4.0805e-01,  4.0770e-01, -2.0958e-01,  3.7544e-01,  2.0993e-01,\n",
      "         -8.9118e-02,  1.2008e-02, -3.4833e-01, -5.2058e-02, -1.5351e-01,\n",
      "          4.0158e+00,  1.6235e-01,  3.8779e-02, -8.8957e-02,  1.0436e-01,\n",
      "         -1.3521e-02,  4.8219e-01,  4.5837e-01, -3.9977e-01,  4.9098e-01,\n",
      "         -2.4416e-01,  3.0334e-01,  6.6008e-02, -3.7856e-02, -1.1618e-01,\n",
      "          2.0705e-01, -1.1884e-01, -3.2601e-01,  5.6063e-02, -6.6303e-01,\n",
      "          4.5335e-01,  2.1872e-01,  5.4113e-01,  3.7996e-01, -1.3454e+00,\n",
      "          3.8308e-01, -2.7423e-01, -3.2941e-01,  3.4648e-02, -2.3437e-01,\n",
      "         -6.7488e-02,  4.0508e-01, -2.6226e-01,  4.0666e-01, -2.1975e-01,\n",
      "         -1.3625e-01,  4.9299e-01, -6.8814e-02, -1.4089e-01,  1.1150e-02,\n",
      "         -1.5895e-01,  2.8983e-01, -4.8845e-01,  1.8385e-01, -1.1898e-01,\n",
      "          4.5387e-01,  5.7814e-02, -4.1368e-01, -5.4490e-01, -4.4490e-02,\n",
      "          1.2731e-01, -7.1993e-04,  2.6185e-02, -5.6105e-01,  4.4852e-01,\n",
      "         -2.4274e-01, -1.5467e-01,  7.6156e-03, -5.8924e-02, -6.9214e-01,\n",
      "         -1.6775e-01, -4.6424e-01,  1.7934e-01,  2.6244e-01,  2.8710e-02,\n",
      "          1.7368e-01, -8.4839e-01, -2.4264e-01, -3.4398e+00, -2.4796e-01,\n",
      "         -1.4297e-01,  5.6377e-01,  5.1424e-01, -4.1335e-01,  7.5439e-02,\n",
      "          2.2955e-01,  2.8838e-01, -2.4855e-01,  3.6572e-01,  5.9075e-02,\n",
      "         -2.0086e-01,  4.3800e-02, -4.4958e-01,  3.1884e-01,  6.5272e-02,\n",
      "         -9.8554e-02, -8.4821e-02,  2.0189e-02,  3.9734e-01,  1.2418e-01,\n",
      "         -2.0505e-01,  8.7679e-02,  1.5165e-01, -5.5344e-01, -3.5472e-01,\n",
      "         -4.7118e-01, -3.0615e-01, -4.1562e-01, -1.5634e-01,  5.5686e-02,\n",
      "         -3.1934e-02,  7.3355e-02,  1.1565e-01, -2.9241e+00,  3.1085e-01,\n",
      "          1.1438e-01, -7.9927e-02, -2.1139e-01, -3.1754e-01,  1.9568e-01,\n",
      "          2.1696e-01, -1.1464e-01,  2.4021e-01,  5.7807e-02, -3.3126e-01,\n",
      "         -2.8130e-01,  3.3144e-01,  3.0261e-01,  8.9073e-02,  3.2097e-01,\n",
      "          2.9475e-01,  2.6480e-01, -3.4610e-02, -4.0923e-01,  6.1893e-03,\n",
      "          1.9558e-02, -1.6176e-01, -4.1628e-02,  1.9690e-01, -6.3301e-01,\n",
      "          6.5197e-02, -4.5425e-01, -3.4340e-01, -1.5985e-01, -1.0818e-02,\n",
      "          2.1636e-01, -2.7393e-01, -6.8339e-01, -1.0950e-01,  2.6826e-01,\n",
      "          5.1804e-02,  6.7454e-01, -3.6607e-01,  4.8469e-02,  1.1338e+00,\n",
      "          3.1416e-01,  3.1541e-01,  4.5882e-01, -9.8268e-02, -6.5379e-02,\n",
      "          4.1940e-02,  4.5828e-01, -2.5445e-01, -5.6839e-01,  6.9147e-02,\n",
      "          9.9380e-01, -2.9112e-01,  3.1405e-01, -2.5835e-01,  6.8054e-01,\n",
      "          3.0407e-01,  1.2964e-02,  1.5425e-01,  1.1031e+00, -3.4439e-01,\n",
      "          5.6558e-01, -5.7273e-01, -5.7190e-02, -4.1406e-01,  3.2958e-01,\n",
      "         -1.9676e-01,  5.9560e-02,  4.0836e-01, -5.4384e-01,  5.4862e-01,\n",
      "         -1.7408e-01, -1.3385e+00, -7.3902e-03, -1.4017e-01, -1.6494e-01,\n",
      "         -4.6694e-02,  2.2153e-01, -2.6778e-01, -3.2321e-01, -2.3508e-01,\n",
      "         -3.2678e-01,  4.7900e-01,  8.7328e-02,  3.6017e-02, -1.7747e-01,\n",
      "          6.5438e-02, -8.3209e-01,  1.4382e-01, -1.3062e-01,  5.2388e-01,\n",
      "          1.4552e-01,  2.4736e-01, -3.6399e-01,  3.5158e-01,  5.6317e-01,\n",
      "         -5.2237e-01,  3.2470e-01, -3.3202e-01, -2.6398e-02, -4.3758e-01,\n",
      "         -5.5893e-01, -3.6261e-02, -3.6732e-01, -2.3899e-01, -3.4811e-01,\n",
      "          2.6370e-01,  5.8845e-01,  1.7416e-01, -3.0619e-01,  8.7868e-01,\n",
      "         -4.5382e-02,  5.5797e-02,  6.0148e-01,  1.0252e-01, -1.9734e-01,\n",
      "          6.8076e-01,  5.7100e-01,  2.4307e-01,  1.0309e-01,  5.0644e-01,\n",
      "          3.0187e-01, -6.1227e-02, -2.4397e-01, -3.4507e-01,  4.1183e-01,\n",
      "         -7.7365e-02, -5.9858e-01, -3.7121e-01, -5.0681e-02, -1.4672e-01,\n",
      "         -4.0111e-01, -5.2969e-01, -2.1027e-01, -7.5360e-01, -5.3347e-02,\n",
      "          9.3595e-01,  4.4910e-01, -4.6546e-01,  7.2688e-01,  1.4760e-01,\n",
      "         -2.9289e-01,  7.4066e-01, -5.0327e-02,  7.3789e-01,  2.0059e-02,\n",
      "         -4.0013e-02, -3.8184e-01,  6.6657e-01,  1.0430e-01, -4.1015e-01,\n",
      "          1.7685e-01, -5.5298e-01,  9.3074e-01, -2.1628e-01, -4.0546e-01,\n",
      "         -1.1694e-01,  5.3974e-02, -5.1438e-01,  2.6972e-01,  4.9425e-01,\n",
      "         -1.4538e+00,  5.6881e-02,  1.9779e-01, -2.1924e-01,  2.4452e-01,\n",
      "         -1.0562e-01, -6.0184e-01,  6.8750e-01, -1.1205e-01, -2.4148e-01,\n",
      "         -3.4798e-01, -1.7863e-01, -3.7464e-02,  3.7303e-01,  3.4793e-01,\n",
      "         -1.5197e-01,  3.8102e-01, -5.0176e-02,  1.7980e-01,  9.0585e-02,\n",
      "          2.8118e-02,  3.6663e-01,  2.9235e-01, -2.4943e-01,  2.0716e-01,\n",
      "         -6.0778e-02, -2.9051e-01,  9.5562e-02, -1.0807e-01,  1.7150e-01,\n",
      "          2.1165e-01, -4.1636e-01, -4.9613e-01,  2.0454e-01,  5.0064e-01,\n",
      "          2.0990e-01,  1.1688e-01, -6.1914e-01,  6.2636e-01,  3.9035e-01,\n",
      "         -6.1850e-01,  4.0555e-01,  5.0704e-02, -2.5509e-01,  4.0812e-01,\n",
      "          4.9067e-01, -4.1513e-01, -1.5604e-02, -1.4188e-01, -5.5191e-01,\n",
      "         -2.4521e-02, -4.3549e-01, -9.0578e-02, -2.4309e-01,  4.2149e-01,\n",
      "         -4.5150e-01, -1.5444e-01, -1.0271e-01, -3.3354e-01, -2.4609e-02,\n",
      "          3.9645e-01,  6.0441e-02,  1.2666e-01,  1.7969e-01, -5.7727e-01,\n",
      "         -7.9196e-01,  3.7416e-01, -5.2998e-01, -3.4937e-01,  5.1382e-04,\n",
      "          1.8590e-01,  3.5871e-01, -2.7055e-01,  5.9520e-02, -5.2192e-01,\n",
      "          2.9779e-01, -2.5002e-01, -5.3179e-01,  4.7002e-01, -1.8759e-01,\n",
      "         -1.0597e-02, -5.2976e-01, -1.7018e-01,  2.4935e-01, -8.4805e-02,\n",
      "          3.5066e-02, -2.2261e-01, -2.8295e-01,  1.9896e-01,  2.3151e-01,\n",
      "         -4.0711e-01, -3.1032e-01,  7.3527e-01,  2.1899e-01,  5.4541e-02,\n",
      "         -2.8997e-01, -4.0908e-01, -1.4038e-01,  1.9831e-01, -9.8112e-02,\n",
      "         -4.2022e-01,  2.0146e-01,  6.5857e-01,  4.6403e-01,  9.2361e-02,\n",
      "          2.4361e-01,  5.3967e-01,  3.0906e-01, -4.5505e-01, -2.1207e-01,\n",
      "          1.2071e-01,  1.0619e-01,  2.0079e-01, -4.3239e-01,  4.0601e-01,\n",
      "          1.4422e-01,  2.4686e-01, -4.3320e-01,  2.0087e+00,  2.3266e-01,\n",
      "          1.2663e-01,  1.6836e-01,  6.3371e-01, -1.2037e-01,  1.1057e-01,\n",
      "         -6.2856e-02, -1.9677e-01,  5.0390e-01, -1.9811e-01,  3.0866e-01,\n",
      "         -2.2943e-01,  2.5207e-01,  1.2665e-01,  7.0377e-01, -2.0744e-01,\n",
      "         -3.6623e-01, -7.1009e-01,  2.0967e-01, -3.8629e-01,  8.1813e-01,\n",
      "         -2.3941e-02, -5.7654e-02,  3.2608e-01,  4.2669e-02,  3.1683e-01,\n",
      "         -1.3152e-01,  5.1694e-02,  2.2050e-02, -1.7904e-01,  5.3066e-01,\n",
      "          1.4196e-01,  1.6110e-01, -2.6057e-01, -4.0975e-02,  9.3228e-02,\n",
      "         -2.5060e-01, -6.9451e-02, -3.7165e-02, -1.1510e-02, -4.9115e-01,\n",
      "          9.5258e-01,  1.9512e-01, -1.7386e-02,  6.7308e-01, -8.4652e-02,\n",
      "         -4.6781e-01,  2.0459e-01,  1.3128e-02,  2.2799e-01, -1.9985e-01,\n",
      "         -5.6316e-01,  2.0072e-01, -4.3228e-01, -1.6256e-01,  3.1343e-01,\n",
      "          1.8822e-04, -1.2932e-01,  2.9119e-01,  2.7400e-01,  5.2320e-01,\n",
      "          2.0613e-01, -6.7237e-01, -6.7378e-01, -4.1236e-01, -2.1954e-01,\n",
      "          5.2282e-01, -1.8860e-01,  2.9475e-01,  3.2144e-01,  1.4274e-01,\n",
      "          4.2647e-01,  5.8457e-04,  2.9863e-01,  2.2948e-01,  1.9769e-02,\n",
      "          2.7079e-01, -2.9118e-01, -2.6266e+00,  1.4987e-01,  1.8488e-01,\n",
      "          3.6244e-02, -4.7328e-02,  2.9595e-01, -2.7069e-01,  2.4112e-01,\n",
      "          2.7055e-01,  9.2248e-03,  1.4236e-01,  2.2307e-01,  3.4398e-01,\n",
      "          3.3338e-01,  3.3243e-02,  2.8387e-01,  2.2436e-01, -9.7830e-02,\n",
      "         -2.9460e-03, -3.4570e-01,  2.4340e-01,  9.9630e-03, -2.2392e-02,\n",
      "          2.5013e-01, -6.8149e-01,  2.0660e-01,  1.5169e-02, -1.7532e-01,\n",
      "          3.0356e-01,  1.3596e-01, -2.3591e-01, -3.8980e-02,  7.1184e-02,\n",
      "          2.1471e-01,  4.5234e-02,  3.3489e-02,  1.6065e-01, -1.2959e-01,\n",
      "          5.0116e-01,  5.5216e-01, -4.0884e-01,  2.8541e-01, -3.0322e-01,\n",
      "          3.9034e-01, -2.9374e-01, -2.2683e-01,  2.6648e-01, -4.3546e-01,\n",
      "          9.4370e-01, -2.9002e-01,  1.3778e-01,  3.4912e-01, -7.5152e-02,\n",
      "         -5.1108e-02,  2.8832e-01, -4.1685e-02,  2.4589e-01,  2.1107e-01,\n",
      "          1.4001e-01,  1.8270e-01, -8.6713e-02, -1.0819e-01, -3.4170e-01,\n",
      "          1.6110e-01,  2.7895e-01, -2.2077e-01,  1.5235e-01,  4.7660e-02,\n",
      "          3.3009e-01, -2.4280e-01,  1.2283e-02, -4.7748e-02,  6.3715e-01,\n",
      "          7.9378e-02,  1.1215e-01,  1.5307e-01,  3.2482e-01,  1.9940e-01,\n",
      "          6.4368e-01,  8.1171e-01, -1.8955e-01, -5.6073e-01, -2.1649e-01,\n",
      "          2.7116e-01,  4.2149e-01, -7.1591e+00, -1.6932e-01, -4.2588e-01,\n",
      "         -3.2319e-01, -4.6207e-01, -4.6374e-01,  2.4044e-01, -3.3461e-01,\n",
      "          1.5050e-01, -5.6236e-02, -2.0050e-01,  2.3312e-01,  3.1556e-01,\n",
      "         -6.1209e-01,  2.1832e-02,  9.0609e-01]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"NLP embeddings capture meaning\"\n",
    "inputs = tokenizer(text , return_tensors=\"pt\")\n",
    "\n",
    " # model se embedding vector nikalta hai\n",
    "\n",
    "outputs = model(**inputs)   \n",
    "cls_vector = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "\n",
    "\n",
    "print(cls_vector)                       # vector jis me sentence ka meaning store hota hai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e447e9",
   "metadata": {},
   "source": [
    "### 3# Sentence Similarity Project :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93e323",
   "metadata": {},
   "source": [
    "* Sentence similarity project me do sentences ke beech meaning ka kitna close relation hai wo calculate kiya jata hai.\n",
    "\n",
    "* Model embeddings generate karta hai aur un vectors ka distance compare karke similarity score deta hai.\n",
    "\n",
    "* Ye project semantic understanding test karta hai yani model sirf words nahi poore sentence ka idea samajhta hai.\n",
    "\n",
    "* Similarity score 0 to 1 ya cosine similarity ke form me hota hai jisse pata lagta hai ki text kitna similar hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea95da",
   "metadata": {},
   "source": [
    "* Why in AI/ML (4 points) : \n",
    "\n",
    "    - Sentence similarity applications me essential hota hai jaise duplicate question detection, search ranking, recommendation.\n",
    "\n",
    "    - NLP systems ke quality measure karne ka best method hota hai ki sentences ka meaning kitna accurately match hota hai.\n",
    "\n",
    "    - AI ko real life text relation samajhna padta hai aur similarity task uska practical test hota hai.\n",
    "\n",
    "    - Ye project embeddings aur transformer models ka actual utility dikhata hai jo ML learning ko strong banata hai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0333034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.6640487909317017\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer , util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\") # pretrained embedding model\n",
    "\n",
    "s1 = \"I love learning NLP\"\n",
    "s2 = \"Studying natural language processing is enjoyable\"\n",
    "\n",
    "e1 = model.encode(s1)  # first sentence embedding\n",
    "e2 = model.encode(s2)  # second sentence embedding\n",
    "\n",
    "score = util.cos_sim(e1, e2)  # cosine similarity to compare meaning\n",
    "\n",
    "print(\"similarity:\", score.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
